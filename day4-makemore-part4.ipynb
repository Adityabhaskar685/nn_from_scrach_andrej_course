{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Manual Backprogation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch \nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:23.541399Z","iopub.execute_input":"2023-04-18T09:57:23.542846Z","iopub.status.idle":"2023-04-18T09:57:24.818636Z","shell.execute_reply.started":"2023-04-18T09:57:23.542777Z","shell.execute_reply":"2023-04-18T09:57:24.817296Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"words = open('/kaggle/input/nn-zere-hero/names.txt', 'r').read().splitlines()\nlen(words)\nmax(len(w) for w in words)\nwords[:10]","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:24.821226Z","iopub.execute_input":"2023-04-18T09:57:24.822625Z","iopub.status.idle":"2023-04-18T09:57:24.851304Z","shell.execute_reply.started":"2023-04-18T09:57:24.822542Z","shell.execute_reply":"2023-04-18T09:57:24.850294Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']"},"metadata":{}}]},{"cell_type":"code","source":"chars = sorted(list(set(''.join(words))))\nstoi = {s : i+1 for i, s in enumerate(chars)}\nstoi[\".\"] = 0\nitos = {s : i for i, s in stoi.items()}\nprint(itos)\nvocab_size = len(itos)\nprint('vocabulary_size: ', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:24.852957Z","iopub.execute_input":"2023-04-18T09:57:24.853328Z","iopub.status.idle":"2023-04-18T09:57:24.867306Z","shell.execute_reply.started":"2023-04-18T09:57:24.853292Z","shell.execute_reply":"2023-04-18T09:57:24.865714Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\nvocabulary_size:  27\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 3 # context length\n\ndef build_dataset(words):\n    X = []\n    Y = []\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            \n            context = context[1:] + [ix] # crop and append\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X,Y\n\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\nn1 = int(len(words) * 0.8)\nn2 = int(len(words) * 0.9)\n\nXtr , Ytr = build_dataset(words[:n1]) # training set 80%\nXval,  Yval = build_dataset(words[n1:n2]) # validation set # 10%\nXte, Yte = build_dataset(words[n2:])  # test set # 10%","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:24.871764Z","iopub.execute_input":"2023-04-18T09:57:24.872792Z","iopub.status.idle":"2023-04-18T09:57:25.585208Z","shell.execute_reply.started":"2023-04-18T09:57:24.872729Z","shell.execute_reply":"2023-04-18T09:57:25.583867Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n","output_type":"stream"}]},{"cell_type":"code","source":"# utility function for comparing manual gradients to pytorch gradients\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.587151Z","iopub.execute_input":"2023-04-18T09:57:25.587713Z","iopub.status.idle":"2023-04-18T09:57:25.596233Z","shell.execute_reply.started":"2023-04-18T09:57:25.587643Z","shell.execute_reply":"2023-04-18T09:57:25.594350Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# MLP \nn_embd = 10 # the dimentionality of character embedding vectors\nn_hidden =  64 # number of neurons in the hidden layer\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibily\nC = torch.randn((vocab_size, n_embd),    generator = g)\n\n# layer1 \nW1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3)/((n_embd * block_size) **0.5) # kiminig initialization\nb1 = torch.randn(n_hidden,       generator = g) * 0.1 # using b1 anyway, it's useless becuase of batch normalization\n\n# layer2\nW2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\nb2 = torch.randn(vocab_size ,     generator = g)\n\n# BatchNorm parameter\nbngain = torch.randn((1,n_hidden)) *0.1 + 1.0\nbnbias = torch.randn((1,n_hidden)) * 0.1\n\n\n# note: Initializing many of these parameters in non-standard ways\n# because sometimes initializing with e.g all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:35.546855Z","iopub.execute_input":"2023-04-18T10:15:35.547315Z","iopub.status.idle":"2023-04-18T10:15:35.560102Z","shell.execute_reply.started":"2023-04-18T10:15:35.547277Z","shell.execute_reply":"2023-04-18T10:15:35.558253Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"4137\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 32\nn = batch_size # a shorter variable just for convinece\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\nXb , Yb = Xtr[ix], Ytr[ix] # batch X, Y\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:37.426966Z","iopub.execute_input":"2023-04-18T10:15:37.427396Z","iopub.status.idle":"2023-04-18T10:15:37.435475Z","shell.execute_reply.started":"2023-04-18T10:15:37.427360Z","shell.execute_reply":"2023-04-18T10:15:37.433698Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0],-1) # concatenate the vectors\n#Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim = True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff ** 2\nbnvar = 1 / (n-1)* (bndiff2).sum(0, keepdim = True) # note: Bessel's correction (diving by n-1, not n)\nbnvar_inv = (bnvar + 1e-5) **-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# hidden layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim = True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical statbility\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdim = True)\ncounts_sum_inv = counts_sum**-1 # if I use(1.0 /counts_sum) instead then I can't get backprop to bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# Pytorch backward pass\nfor p in parameters:\n    p.grad = None\n    \nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, #africk there is no cleaner war\n         norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n    t.retain_grad()\nloss.backward()\nloss\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:39.302416Z","iopub.execute_input":"2023-04-18T10:15:39.303747Z","iopub.status.idle":"2023-04-18T10:15:39.325405Z","shell.execute_reply.started":"2023-04-18T10:15:39.303680Z","shell.execute_reply":"2023-04-18T10:15:39.324358Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor(3.8419, grad_fn=<NegBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"emb.shape, C.shape, Xb.shape\nXb[:5]","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.844344Z","iopub.execute_input":"2023-04-18T09:57:25.844746Z","iopub.status.idle":"2023-04-18T09:57:25.853301Z","shell.execute_reply.started":"2023-04-18T09:57:25.844700Z","shell.execute_reply":"2023-04-18T09:57:25.851845Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1,  1,  4],\n        [18, 14,  1],\n        [11,  5,  9],\n        [ 0,  0,  1],\n        [12, 15, 14]])"},"metadata":{}}]},{"cell_type":"code","source":"hprebn.shape, embcat.shape, W1.shape, b1.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.856807Z","iopub.execute_input":"2023-04-18T09:57:25.857447Z","iopub.status.idle":"2023-04-18T09:57:25.865489Z","shell.execute_reply.started":"2023-04-18T09:57:25.857406Z","shell.execute_reply":"2023-04-18T09:57:25.864511Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 64]),\n torch.Size([32, 30]),\n torch.Size([30, 64]),\n torch.Size([64]))"},"metadata":{}}]},{"cell_type":"code","source":"dembcat = dhprebn @ W1.T\ndw1 = embcat.T @ dhprebn\nb1 = dhpredn.sum(0)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.867539Z","iopub.execute_input":"2023-04-18T09:57:25.868208Z","iopub.status.idle":"2023-04-18T09:57:25.949755Z","shell.execute_reply.started":"2023-04-18T09:57:25.868153Z","shell.execute_reply":"2023-04-18T09:57:25.947888Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/4109395565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdembcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdhprebn\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdhprebn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdhpredn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dhprebn' is not defined"],"ename":"NameError","evalue":"name 'dhprebn' is not defined","output_type":"error"}]},{"cell_type":"code","source":"dbnvar_inv = (bndiff * dbnrawa).sum(0, keepdim = True)\ndbndiff = bnvar * dbnraw","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.951163Z","iopub.status.idle":"2023-04-18T09:57:25.952558Z","shell.execute_reply.started":"2023-04-18T09:57:25.952178Z","shell.execute_reply":"2023-04-18T09:57:25.952218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dbngain = (bnraw * hpreact).sum(0, keepdim = True)\ndbnraw = bngain * hpreact\ndbnbias = dhpreact.sum(0, keepdim = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T09:57:25.954733Z","iopub.status.idle":"2023-04-18T09:57:25.955391Z","shell.execute_reply.started":"2023-04-18T09:57:25.955068Z","shell.execute_reply":"2023-04-18T09:57:25.955103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Exercise 1: backprop through the whole thing manually\n# backpropagating through exactly all of the variables\n# as they are defined in the forward pass above,one by one\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ndprobs = (1.0/ probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-(counts_sum**-2) * dcounts_sum_inv)\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim = True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]) * dlogit_maxes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\ndhpreact = (1.0 - h**2) * dh\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\ndbnmeani = (-dbndiff).sum(0)\ndhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(emb.shape)\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n    for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j] \n\ncmp('dlogprobs', dlogprobs, logprobs)\ncmp('dprobs', dprobs, probs)\ncmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp(\"dcounts_sum\", dcounts_sum, counts_sum)\ncmp('dcounts', dcounts, counts)\ncmp('dnorm_logits',dnorm_logits,norm_logits)\ncmp('dlogit_maxes', dlogit_maxes, logit_maxes)\ncmp('dlogits',dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:45.497551Z","iopub.execute_input":"2023-04-18T10:15:45.497977Z","iopub.status.idle":"2023-04-18T10:15:45.531469Z","shell.execute_reply.started":"2023-04-18T10:15:45.497941Z","shell.execute_reply":"2023-04-18T10:15:45.530425Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"dlogprobs       | exact: True  | approximate: True  | maxdiff: 0.0\ndprobs          | exact: True  | approximate: True  | maxdiff: 0.0\ndcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\ndcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\ndcounts         | exact: True  | approximate: True  | maxdiff: 0.0\ndnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\ndlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\ndlogits         | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\nbngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\nbndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nhprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nembcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nW1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\nb1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\nemb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nC               | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n","output_type":"stream"}]},{"cell_type":"code","source":"# Exercise2 : backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simply the expression, and just write it out\n\n\n# forward pass\n\n# before\n# logit_maxes = logits.max(1, keepdim = True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical statbility\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdim = True)\n# counts_sum_inv = counts_sum**-1 # if I use(1.0 /counts_sum) instead then I can't get backprop to bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), 'diff: ', (loss_fast - loss).item())","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:51.298974Z","iopub.execute_input":"2023-04-18T10:15:51.299388Z","iopub.status.idle":"2023-04-18T10:15:51.308070Z","shell.execute_reply.started":"2023-04-18T10:15:51.299355Z","shell.execute_reply":"2023-04-18T10:15:51.306697Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"3.8418736457824707 diff:  0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# backward pass\n\ndlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1\ndlogits /= n\n\ncmp('dlogits', dlogits, logits)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:15:53.755465Z","iopub.execute_input":"2023-04-18T10:15:53.755893Z","iopub.status.idle":"2023-04-18T10:15:53.765088Z","shell.execute_reply.started":"2023-04-18T10:15:53.755852Z","shell.execute_reply":"2023-04-18T10:15:53.763343Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"dlogits         | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n","output_type":"stream"}]},{"cell_type":"code","source":"F.softmax(logits, 1)[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:16:02.923301Z","iopub.execute_input":"2023-04-18T10:16:02.924263Z","iopub.status.idle":"2023-04-18T10:16:02.935325Z","shell.execute_reply.started":"2023-04-18T10:16:02.924217Z","shell.execute_reply":"2023-04-18T10:16:02.934038Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([0.0865, 0.0193, 0.0128, 0.0181, 0.0061, 0.1944, 0.0696, 0.0672, 0.0147,\n        0.0088, 0.0165, 0.0064, 0.0598, 0.0120, 0.0059, 0.0118, 0.0042, 0.0043,\n        0.0067, 0.1342, 0.0528, 0.0038, 0.0260, 0.0344, 0.1101, 0.0102, 0.0034],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"dlogits[0] * n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:16:04.155039Z","iopub.execute_input":"2023-04-18T10:16:04.155504Z","iopub.status.idle":"2023-04-18T10:16:04.165473Z","shell.execute_reply.started":"2023-04-18T10:16:04.155465Z","shell.execute_reply":"2023-04-18T10:16:04.163653Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.0865,  0.0193,  0.0128,  0.0181,  0.0061,  0.1944,  0.0696,  0.0672,\n        -0.9853,  0.0088,  0.0165,  0.0064,  0.0598,  0.0120,  0.0059,  0.0118,\n         0.0042,  0.0043,  0.0067,  0.1342,  0.0528,  0.0038,  0.0260,  0.0344,\n         0.1101,  0.0102,  0.0034], grad_fn=<MulBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"plt.figure(figsize =(5,5))\nplt.imshow(dlogits.detach(), cmap = 'gray')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:16:04.651619Z","iopub.execute_input":"2023-04-18T10:16:04.652963Z","iopub.status.idle":"2023-04-18T10:16:04.903591Z","shell.execute_reply.started":"2023-04-18T10:16:04.652914Z","shell.execute_reply":"2023-04-18T10:16:04.902030Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7616d6651c10>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n# BatchNorm paper: https://arxiv.org/abs/1502.03167\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:16:08.866216Z","iopub.execute_input":"2023-04-18T10:16:08.866702Z","iopub.status.idle":"2023-04-18T10:16:08.876936Z","shell.execute_reply.started":"2023-04-18T10:16:08.866662Z","shell.execute_reply":"2023-04-18T10:16:08.875519Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# backward pass\n\n# before we had:\n# dbnraw = bngain * dhpreact\n# dbndiff = bnvar_inv * dbnraw\n# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n# dbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n\n# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n# (you'll also need to use some of the variables from the forward pass up above)\n\n# -----------------\n\ndhprebn = bngain * bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact * bnraw).sum(0))\n# -----------------\n\ncmp('hprebn', dhprebn, hprebn)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:16:20.001172Z","iopub.execute_input":"2023-04-18T10:16:20.001591Z","iopub.status.idle":"2023-04-18T10:16:20.011338Z","shell.execute_reply.started":"2023-04-18T10:16:20.001556Z","shell.execute_reply":"2023-04-18T10:16:20.010187Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n","output_type":"stream"}]},{"cell_type":"code","source":"# Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\n\n# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n    # kick off optimization\n    for i in range(max_steps):\n        # minibatch construct\n        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n        # forward pass\n        emb = C[Xb] # embed the characters into vectors\n        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n        # Linear layer\n        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n        # BatchNorm layer\n        # -------------------------------------------------------------\n        bnmean = hprebn.mean(0, keepdim=True)\n        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n        bnvar_inv = (bnvar + 1e-5)**-0.5\n        bnraw = (hprebn - bnmean) * bnvar_inv\n        hpreact = bngain * bnraw + bnbias\n        # -------------------------------------------------------------\n        # Non-linearity\n        h = torch.tanh(hpreact) # hidden layer\n        logits = h @ W2 + b2 # output layer\n        loss = F.cross_entropy(logits, Yb) # loss function\n\n        # backward pass\n        for p in parameters:\n            p.grad = None\n        #loss.backward() # use this for correctness comparisons, delete it later!\n\n        # manual backprop! #swole_doge_meme\n        # -----------------\n        dlogits = F.softmax(logits, 1)\n        dlogits[range(n), Yb] -= 1\n        dlogits /= n\n        dh = dlogits @ W2.T\n        dW2 = h.T @ dlogits\n        db2 = dlogits.sum(0)\n        dhpreact = (1.0 - h**2) * dh\n        dhprebn = bngain * bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact * bnraw).sum(0))\n        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n        dbnbias = dhpreact.sum(0, keepdim=True)\n        dembcat = dhprebn @ W1.T\n        dW1 = embcat.T @ dhprebn\n        db1 = dhprebn.sum(0)\n        demb = dembcat.view(emb.shape)\n        dC = torch.zeros_like(C)\n        for k in range(Xb.shape[0]):\n            for j in range(Xb.shape[1]):\n                ix = Xb[k,j]\n                dC[ix] += demb[k,j]\n        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n        # -----------------\n\n        # update\n        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n        for p, grad in zip(parameters, grads):\n            #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n            p.data += -lr * grad # new way of swole doge TODO: enable\n\n        # track stats\n        if i % 10000 == 0: # print every once in a while\n            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n        lossi.append(loss.log10().item())\n\n    #     if i >= 10000: # TODO: delete early breaking when you're ready to train the full net\n    #         break","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:33:18.988329Z","iopub.execute_input":"2023-04-18T10:33:18.988778Z","iopub.status.idle":"2023-04-18T10:40:46.601801Z","shell.execute_reply.started":"2023-04-18T10:33:18.988738Z","shell.execute_reply":"2023-04-18T10:40:46.600652Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"12297\n      0/ 200000: 3.7961\n  10000/ 200000: 2.1493\n  20000/ 200000: 2.4002\n  30000/ 200000: 2.4349\n  40000/ 200000: 1.9819\n  50000/ 200000: 2.3766\n  60000/ 200000: 2.4128\n  70000/ 200000: 2.0216\n  80000/ 200000: 2.3025\n  90000/ 200000: 2.1236\n 100000/ 200000: 1.9366\n 110000/ 200000: 2.3024\n 120000/ 200000: 1.9163\n 130000/ 200000: 2.4490\n 140000/ 200000: 2.3989\n 150000/ 200000: 2.1295\n 160000/ 200000: 1.9918\n 170000/ 200000: 1.8317\n 180000/ 200000: 2.0077\n 190000/ 200000: 1.9071\n","output_type":"stream"}]},{"cell_type":"code","source":"# useful for checking your gradients\n# for p,g in zip(parameters, grads):\n#       cmp(str(tuple(p.shape)), g, p)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:18:18.275454Z","iopub.execute_input":"2023-04-18T10:18:18.275890Z","iopub.status.idle":"2023-04-18T10:18:18.287612Z","shell.execute_reply.started":"2023-04-18T10:18:18.275852Z","shell.execute_reply":"2023-04-18T10:18:18.285434Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"(27, 10)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n(30, 200)       | exact: False | approximate: True  | maxdiff: 1.210719347000122e-08\n(200,)          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n(27,)           | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n(1, 200)        | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n","output_type":"stream"}]},{"cell_type":"code","source":"# calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[Xtr]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the entire training set\n    bnmean = hpreact.mean(0, keepdim=True)\n    bnvar = hpreact.var(0, keepdim=True, unbiased=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:42:07.796082Z","iopub.execute_input":"2023-04-18T10:42:07.796518Z","iopub.status.idle":"2023-04-18T10:42:08.231824Z","shell.execute_reply.started":"2023-04-18T10:42:07.796482Z","shell.execute_reply":"2023-04-18T10:42:08.230699Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# evaluate train and val loss\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xval, Yval),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:42:12.448996Z","iopub.execute_input":"2023-04-18T10:42:12.449471Z","iopub.status.idle":"2023-04-18T10:42:13.245014Z","shell.execute_reply.started":"2023-04-18T10:42:12.449433Z","shell.execute_reply":"2023-04-18T10:42:13.243679Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"train 2.071704387664795\nval 2.1113476753234863\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample from the model\ng = torch.Generator().manual_seed(2147483647 +10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # forward pass\n        emb = C[torch.tensor([context])] # (1,block_size,d)      \n        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n        hpreact = embcat @ W1 + b1\n        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n        h = torch.tanh(hpreact) # (N, n_hidden)\n        logits = h @ W2 + b2 # (N, vocab_size)\n        # sample\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n\n    print(''.join(itos[i] for i in out))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T10:42:42.492482Z","iopub.execute_input":"2023-04-18T10:42:42.492998Z","iopub.status.idle":"2023-04-18T10:42:42.534702Z","shell.execute_reply.started":"2023-04-18T10:42:42.492954Z","shell.execute_reply":"2023-04-18T10:42:42.533259Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"carmahzamille.\nkhy.\nmrix.\ntaty.\nsacassie.\nmahnen.\ndelynn.\njareei.\nnellara.\nchaiivon.\nleigh.\nham.\njocn.\nquinn.\nshon.\nmarianni.\nwavero.\ndearyxi.\njace.\npirra.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}