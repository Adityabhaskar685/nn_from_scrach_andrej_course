{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622026e6-b6e6-47db-8a94-1074601af656",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 2 Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "14c9d2fb-cfb9-4055-95e3-ed07d9387bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77405087-bdff-4e46-b5a3-6dd4ab04c00f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72281512-4540-4276-91e2-57d088af0ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d124c-da7e-4b22-944c-b9c836cec495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trigram language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d4133382-08b6-48fc-adf5-3f953bbfcaac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars.insert(0,'.')\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "787509cb-394d-431d-b242-d1494191efdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {s:i for i, s in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f2e83424-80f9-4045-8901-77415c9d19ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(a,b) for a in chars for b in chars if b!=\".\"]\n",
    "pairs.insert(0, (\".\",\".\"))\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d1af258-1744-491e-933f-c61881e06e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ptoi = {pair: pos for pos, pair in enumerate(pairs)}\n",
    "itop = {pos: pair for pair, pos in ptoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3969f908-015f-4320-b9bf-43288ae05040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# creating a 27 by 27 by 27 matrix to count all the number of occurence\n",
    "N = torch.zeros((703,27),dtype = torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6a9bf2b-32db-43cb-b18b-46c279406237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filling the count table with counts of each trigram\n",
    "for word in words:\n",
    "    chs = [\".\",\".\"] + list(word) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        p1, p2 = ptoi[(ch1,ch2)], stoi[ch3]\n",
    "        N[p1,p2] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae027703-eef8-4c73-a28a-a8eab98a27ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return N[ptoi[(char1, char2)], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ebea555f-ee73-4844-8cde-b5ed0cb91c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190, dtype=torch.int32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences(\"a\",\"d\",\"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f1a1afc-ec0e-4f3f-abdb-6d13f50c140e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.figure(figsize=(20,20*20))\n",
    "# plt.imshow(N, cmap = 'Blues')\n",
    "# for i in range(703):\n",
    "#     for j in range(27):\n",
    "#         chstr = \"\".join(itop[i]) + itos[j]\n",
    "#         plt.text(j, i, chstr, ha = 'center', va = 'bottom', color = 'gray')\n",
    "#         plt.text(j,i, N[i,j].item(), ha = 'center', va = 'top', color = 'gray')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca28cfec-5762-4ad9-8f9d-a5785e6c79a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4411, 1306, 1542, 1690, 1532,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  395,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86e4c38f-c529-4901-9590-a4f884044da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = (N+1).float() # smoothing\n",
    "P /=  P.sum(1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "04a5c203-023a-434c-a58a-884f5d2f473b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3dae8455-9f4c-4391-924e-605f0f283088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "jakasid\n",
      "prelay\n",
      "adin\n",
      "kairritoper\n",
      "sathen\n",
      "sameia\n",
      "yanileniassibduinrwin\n",
      "lessiyanayla\n",
      "te\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(10):\n",
    "    prev = ('.','.')\n",
    "    out = []\n",
    "    while True:\n",
    "        idx = torch.multinomial(P[ptoi[prev]], num_samples = 1,replacement = True, generator = g).item()\n",
    "        if idx == 0:\n",
    "            break\n",
    "        out.append(itos[idx])\n",
    "        prev = (prev[1],itos[idx])\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eab4640a-f89a-41e7-9d07-5763164e2e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504651.8750)\n",
      "nll=tensor(504651.8750)\n",
      "trigram average nll : 2.2120\n",
      "bigram average nll: 2.4544\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    w = [\".\"] + [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(w,w[1:],w[2:]):\n",
    "        ix1 = ptoi[(ch1,ch2)]\n",
    "        ix2 = stoi[ch3]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n +=1\n",
    "        \n",
    "        #print(f'{ch1}{ch2}{ch3}: {prob : .4f} {logprob: .4f}')\n",
    "        \n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'trigram average nll :{nll/n : .4f}')\n",
    "print(f'bigram average nll: 2.4544')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76c8eb-d523-4724-9ace-8281745d02ae",
   "metadata": {},
   "source": [
    "## Trigram Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b059ecaf-6864-496d-98ed-1185169b419c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . e\n",
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n"
     ]
    }
   ],
   "source": [
    "# creating a training set of trigram\n",
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = [\".\",\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        p1, p2 = ptoi[(ch1,ch2)], stoi[ch3]\n",
    "        print(ch1,ch2,ch3)\n",
    "        xs.append(p1)\n",
    "        ys.append(p2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9fd36f66-62cd-4fbb-a81a-89c8dab6085b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   5, 143, 351, 339])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6903a056-01f2-40be-aac2-b1cd6bb2f44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f6113c6e-472f-425f-91ec-73899f82855d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes = 703).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7159fc60-9624-4733-a6d4-b1503a7c4bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 703])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bcd4ec50-7396-4e97-8148-6b970357f508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x219827460d0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8fc7311a-a245-4fba-8f4f-260581bbb204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f9842a5b-499b-444a-a45e-b6d6dd3dfc99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7530e-03,  8.5583e-02, -1.9633e+00,  2.7030e-01,  3.5163e-01,\n",
       "          5.6949e-01, -3.5364e-01, -2.9256e-01, -1.2036e-01, -1.2206e+00,\n",
       "         -6.5282e-01,  2.5050e+00, -5.4002e-02,  2.1959e+00,  9.0920e-01,\n",
       "         -1.8809e-02, -2.4863e-01,  5.7408e-01,  3.0538e-01,  6.8942e-01,\n",
       "          1.3405e-01,  6.5787e-03, -7.0764e-01, -7.3174e-01, -1.0403e+00,\n",
       "          5.7320e-01, -3.1410e-01],\n",
       "        [-6.2108e-02, -3.4693e-01,  1.0509e+00,  1.1220e+00, -1.6754e+00,\n",
       "          5.6448e-01, -1.2730e+00,  6.8594e-01, -8.1767e-01,  1.8345e+00,\n",
       "          7.5463e-01,  1.2477e+00, -4.5515e-01,  1.4485e-01,  1.5157e+00,\n",
       "          1.4976e+00, -2.1860e-01, -7.0426e-01, -7.2626e-01,  8.9760e-01,\n",
       "          1.3014e+00, -2.5798e+00,  7.1320e-01, -6.1954e-02, -1.2441e+00,\n",
       "         -5.9263e-02,  4.9455e-01],\n",
       "        [-3.0698e-01,  6.8603e-01, -1.5998e-01, -4.5555e-01,  1.2416e+00,\n",
       "         -1.0530e+00,  5.1575e-01, -3.7056e-01,  2.0927e-01,  1.0154e+00,\n",
       "          4.2884e-01,  5.0775e-01, -3.7305e-01,  8.7436e-01,  7.8726e-01,\n",
       "         -1.0670e-01,  7.3021e-01, -1.6156e-01, -1.3748e+00,  5.9087e-01,\n",
       "         -1.6884e+00,  7.9248e-01,  1.3998e+00, -6.9595e-01, -1.0396e+00,\n",
       "         -6.4164e-01,  4.0153e-01],\n",
       "        [ 2.0448e+00,  1.0308e+00,  5.7993e-01, -1.0256e+00,  5.0933e-01,\n",
       "         -7.3827e-01, -1.4672e+00, -1.9289e-02, -9.1596e-01, -1.6668e+00,\n",
       "          2.7610e+00, -6.8999e-01,  5.8518e-01, -3.2770e-01, -1.4229e+00,\n",
       "          5.5521e-01,  8.2106e-01, -2.2127e-01,  5.7983e-02, -1.7155e-01,\n",
       "          1.0263e+00,  1.1525e+00,  5.0988e-01, -2.1481e-01,  1.3721e+00,\n",
       "          4.3238e-01,  1.3558e+00],\n",
       "        [-1.5784e+00,  5.2043e-01, -2.0660e+00, -1.1214e-02,  8.6391e-03,\n",
       "          7.4024e-01,  7.8820e-01,  9.6329e-01, -9.4949e-01, -1.5383e+00,\n",
       "         -4.2803e-01, -6.2320e-01,  1.0192e+00,  6.3058e-01, -8.7184e-01,\n",
       "         -1.5093e-01, -4.5271e-01,  3.7511e-01, -2.3247e-03, -7.4033e-01,\n",
       "          1.8766e+00, -1.7332e-01, -6.1362e-01, -1.1283e+00,  1.5795e+00,\n",
       "          1.9269e-01,  7.0370e-01]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(703,27)\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "12382c4f-06fb-42f0-a121-5aac5981c93b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cd773997-5c0c-4cc4-aa4c-7d570f7c3737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3277)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0f2440c9-ef26-426b-ad1d-b5d3786ce918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons weight, each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((703,27), generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d2b200e6-d0b2-44d8-bbd8-ce68bd57a77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes = 703).float() # input to the network : one_hot\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim = True) # probabilities for next character\n",
    "# btw: the last 2 line here together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "78f27ae4-d50b-45a8-abf8-882bb2091693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dd658067-cc7b-49c0-943a-289dec0d81d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "37c059a0-6c23-43ba-89cf-5e8ca405ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "bigram example 1: ..e (indexes 0, 5)\n",
      "input to the neural net:  0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character) 5\n",
      "probability assigned by the net to the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "----------\n",
      "bigram example 2: .em (indexes 5, 13)\n",
      "input to the neural net:  5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character) 13\n",
      "probability assigned by the net to the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "----------\n",
      "bigram example 3: emm (indexes 143, 13)\n",
      "input to the neural net:  143\n",
      "output probabilities from the neural net: tensor([0.0130, 0.0182, 0.0089, 0.0572, 0.0883, 0.0342, 0.0736, 0.0173, 0.0016,\n",
      "        0.0057, 0.0229, 0.0188, 0.0297, 0.0329, 0.0916, 0.1405, 0.1220, 0.0568,\n",
      "        0.0077, 0.0042, 0.0218, 0.0149, 0.0192, 0.0079, 0.0645, 0.0155, 0.0111])\n",
      "label (actual next character) 13\n",
      "probability assigned by the net to the correct character: 0.03294947370886803\n",
      "log likelihood: -3.4127800464630127\n",
      "negative log likelihood: 3.4127800464630127\n",
      "----------\n",
      "bigram example 4: mma (indexes 351, 1)\n",
      "input to the neural net:  351\n",
      "output probabilities from the neural net: tensor([0.0394, 0.1065, 0.0223, 0.0256, 0.0178, 0.0213, 0.0155, 0.0285, 0.0092,\n",
      "        0.0256, 0.0091, 0.1024, 0.0070, 0.0260, 0.0876, 0.0358, 0.0491, 0.0091,\n",
      "        0.0041, 0.0421, 0.0660, 0.0034, 0.0504, 0.0254, 0.0111, 0.1044, 0.0551])\n",
      "label (actual next character) 1\n",
      "probability assigned by the net to the correct character: 0.10652849823236465\n",
      "log likelihood: -2.23934268951416\n",
      "negative log likelihood: 2.23934268951416\n",
      "----------\n",
      "bigram example 5: ma. (indexes 339, 0)\n",
      "input to the neural net:  339\n",
      "output probabilities from the neural net: tensor([0.0446, 0.0277, 0.0099, 0.0438, 0.0435, 0.0548, 0.0250, 0.0781, 0.0216,\n",
      "        0.0142, 0.0746, 0.1126, 0.0229, 0.0735, 0.0569, 0.0278, 0.0159, 0.0092,\n",
      "        0.0249, 0.0241, 0.0124, 0.0168, 0.0448, 0.0086, 0.0323, 0.0463, 0.0331])\n",
      "label (actual next character) 0\n",
      "probability assigned by the net to the correct character: 0.04463476687669754\n",
      "log likelihood: -3.1092422008514404\n",
      "negative log likelihood: 3.1092422008514404\n",
      "========\n",
      "average negative log likelihood, i.e. loss =  3.1092422008514404\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print(\"----------\")\n",
    "    print(f'bigram example {i+1}: {\"\".join(itop[x])}{itos[y]} (indexes {x}, {y})')\n",
    "    print('input to the neural net: ',x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print(\"label (actual next character)\", y)\n",
    "    p = probs[i,y]\n",
    "    print('probability assigned by the net to the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print(\"========\")\n",
    "print('average negative log likelihood, i.e. loss = ', nll.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3916013-76dd-4640-a3eb-39fef02167eb",
   "metadata": {},
   "source": [
    "training the model on whole trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c36021dc-7de4-4f81-a7e8-69769d26c1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# creating the dataset\n",
    "\n",
    "xs, ys = [],[]\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\",\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        p1, p2 = ptoi[(ch1,ch2)], stoi[ch3]\n",
    "        xs.append(p1)\n",
    "        ys.append(p2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "ns = xs.nelement()\n",
    "print('number of examples: ', ns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f4d82631-2a79-4586-b022-1d30086f1945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xenc = F.one_hot(xs, num_classes = 703).float()\n",
    "W = torch.randn((703,27), generator = g, requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3bc6502d-d1f3-4eef-afb5-3be659fb3b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 703]), torch.Size([703, 27]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "66e2f057-c766-4d4e-8efc-7c45422cec40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0 loss: 3.8183300495147705\n",
      "iteration50 loss: 2.5134758949279785\n",
      "iteration100 loss: 2.387240171432495\n",
      "iteration150 loss: 2.3350751399993896\n",
      "iteration200 loss: 2.3063032627105713\n",
      "iteration250 loss: 2.2881085872650146\n",
      "iteration300 loss: 2.2755985260009766\n",
      "iteration350 loss: 2.2664778232574463\n",
      "iteration400 loss: 2.259530544281006\n",
      "iteration450 loss: 2.254058599472046\n",
      "iteration500 loss: 2.249636173248291\n",
      "iteration550 loss: 2.2459895610809326\n",
      "iteration600 loss: 2.2429332733154297\n",
      "iteration650 loss: 2.2403371334075928\n",
      "iteration700 loss: 2.238107204437256\n",
      "iteration750 loss: 2.2361724376678467\n",
      "iteration800 loss: 2.2344796657562256\n",
      "iteration850 loss: 2.2329864501953125\n",
      "iteration900 loss: 2.2316601276397705\n",
      "iteration950 loss: 2.2304751873016357\n"
     ]
    }
   ],
   "source": [
    "# gradient descent \n",
    "\n",
    "for k in range(1000):\n",
    "    # Forward pass\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim = True)\n",
    "    loss = -probs[torch.arange(ns), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if k % 50 == 0:\n",
    "        print(f\"iteration{k} loss: {loss.item()}\")\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -100 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "43017402-96b7-44a4-8837-5d2712836970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2294304370880127"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b26a1a9b-0820-4e70-8c6c-6464020ebc08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasid\n",
      "prelay\n",
      "adin\n",
      "kairritoper\n",
      "sathen\n",
      "sameia\n",
      "yanileniassibduinewin\n",
      "lessiyanayla\n",
      "te\n"
     ]
    }
   ],
   "source": [
    "# sample from the 'neural net' model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    char1 = '.'\n",
    "    char2 = '.'\n",
    "    \n",
    "    while True: \n",
    "        xenc = F.one_hot(torch.tensor([ptoi[(char1, char2)]]), num_classes = 703).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdim = True) # probabilities for next character\n",
    "        ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        char1 = char2\n",
    "        char2 = itos[ix]\n",
    "        out.append(itos[ix])\n",
    "        \n",
    "    print(''.join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5d74ab1-2bbe-4bba-a63a-866805cd924d",
   "metadata": {},
   "source": [
    "# output from the counting base model\n",
    "\n",
    "junide\n",
    "jakasid\n",
    "prelay\n",
    "adin\n",
    "kairritoper\n",
    "sathen\n",
    "sameia\n",
    "yanileniassibduinrwin\n",
    "lessiyanayla\n",
    "te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7272c-6fa0-46df-ba78-03ac81f0b050",
   "metadata": {},
   "source": [
    " ## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "bea57a35-5375-4372-9fc9-323bf18e281c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 703]), torch.Size([228146]))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes = 703).float()\n",
    "xenc.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "baacc270-87ce-46cb-abb6-8c9acce3a195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182517, 703]) torch.Size([182517])\n",
      "torch.Size([22815, 703]) torch.Size([22815])\n",
      "torch.Size([22814, 703]) torch.Size([22814])\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test set\n",
    "train_idx, val_idx, test_idx = torch.utils.data.random_split(range(xenc.shape[0]), [0.8,0.1,0.1])\n",
    "\n",
    "xs_train , ys_train = xenc[train_idx], ys[train_idx]\n",
    "xs_val , ys_val = xenc[val_idx], ys[val_idx]\n",
    "xs_test, ys_test = xenc[test_idx], ys[test_idx]\n",
    "\n",
    "print(xs_train.shape, ys_train.shape)\n",
    "print(xs_val.shape, ys_val.shape)\n",
    "print(xs_test.shape, ys_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1da7d6a4-d9d5-42aa-8681-2534ba749542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new weights \n",
    "W = torch.rand((703,27), requires_grad = True)\n",
    "R = 0.001 # regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2741b76d-5062-4b55-9328-0a7d433d049c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    # perform matrix multiplication between inputs and weights\n",
    "    logits = inputs @ weights\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(1, keepdim = True)\n",
    "    return p\n",
    "\n",
    "def nll_loss(p, n_inputs, labels, lbd = 0.01):\n",
    "    # calculate the negative log likelihood and apply model smoothing with regularizatin\n",
    "    nll  = -p[torch.arange(n_inputs), labels].log().mean() + lbd * (W**2).mean()\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5bab3f01-8038-4476-8ace-14d5005d7124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation 0, train-loss 2.4068, \t val-loss2.4227\n",
      "interation 100, train-loss 2.3230, \t val-loss2.3424\n",
      "interation 200, train-loss 2.2902, \t val-loss2.3117\n",
      "interation 300, train-loss 2.2717, \t val-loss2.2946\n",
      "interation 400, train-loss 2.2596, \t val-loss2.2838\n",
      "interation 500, train-loss 2.2511, \t val-loss2.2764\n",
      "interation 600, train-loss 2.2448, \t val-loss2.2710\n",
      "interation 700, train-loss 2.2399, \t val-loss2.2669\n",
      "interation 800, train-loss 2.2360, \t val-loss2.2637\n",
      "interation 900, train-loss 2.2328, \t val-loss2.2611\n",
      "interation 1000, train-loss 2.2302, \t val-loss2.2590\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "for i in range(1001):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train)\n",
    "    if i %100 == 0:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val)\n",
    "        print(f'interation {i}, train-loss {loss.item():.4f}, \\t val-loss{val_loss:.4f}')\n",
    "        \n",
    "    # backward_pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "083d6a6e-f95a-4426-8473-85268ce8d8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on test is 2.2576\n"
     ]
    }
   ],
   "source": [
    "# loss on the test set\n",
    "\n",
    "test_loss = nll_loss(forward_pass(xs_test,W), xs_test.shape[0], ys_test)\n",
    "print(f\"loss on test is {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e48ba-5e17-415d-9b6f-c0a715e2d3a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Loss on the validation set and test set in slightly highter than the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e1e96-5eb1-4b08-8f46-78c64e295d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "44d9f0de-6885-4022-89a3-7f312a11e54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {}\n",
    "\n",
    "for i in [1,0.5,0.1,0.05, 0.007, 0.0005, 0.0001,0.00001,0.0000]:\n",
    "    val_prob = forward_pass(xs_val, W)\n",
    "    val_loss = -val_prob[torch.arange(xs_val.shape[0]),ys_val].log().mean() + i * (W**2).mean()\n",
    "    d[i] = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "1eb673c3-a75b-4f21-bcb9-93001928da40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3.0314645767211914,\n",
       " 0.5: 2.6413421630859375,\n",
       " 0.1: 2.329244375228882,\n",
       " 0.05: 2.2902321815490723,\n",
       " 0.007: 2.2566816806793213,\n",
       " 0.0005: 2.251610040664673,\n",
       " 0.0001: 2.251297950744629,\n",
       " 1e-05: 2.251227855682373,\n",
       " 0.0: 2.2512199878692627}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a82c63-d52e-4cdf-8262-1ddeda6828dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "model works best without any regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f7c03040-69eb-422a-bf6b-722c937482f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on test is 2.2498\n"
     ]
    }
   ],
   "source": [
    "# loss on the test set\n",
    "\n",
    "test_loss = nll_loss(forward_pass(xs_test,W), xs_test.shape[0], ys_test,0.0)\n",
    "print(f\"loss on test is {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840591d-6bed-4ffb-ad31-febc7178e73a",
   "metadata": {},
   "source": [
    "## E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "04707941-14c7-4c37-857d-42eb346d4472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182517]) torch.Size([182517])\n",
      "torch.Size([45629]) torch.Size([45629])\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = torch.utils.data.random_split(range(xs.shape[0]),[0.8,0.2])\n",
    "\n",
    "xs_train, ys_train = xs[train_idx], ys[train_idx]\n",
    "xs_val, ys_val = xs[val_idx], ys[val_idx]\n",
    "\n",
    "print(xs_train.shape, ys_train.shape)\n",
    "print(xs_val.shape, ys_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "90d7e92e-616d-4d06-9468-e224fbca2460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    # perform matrix multiplication between inputs and weights\n",
    "    #logits = inputs @ weights\n",
    "    logits = weights[inputs] # inputs consists of indexs\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(1, keepdim = True)\n",
    "    return p\n",
    "\n",
    "def nll_loss(p, n_inputs, labels, lbd = 0.01):\n",
    "    # calculate the negative log likelihood and apply model smoothing with regularizatin\n",
    "    nll  = -p[torch.arange(n_inputs), labels].log().mean() + lbd * (W**2).mean()\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "bdfa1b0c-b442-44b5-8328-d5c0a793737f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation 0, train-loss 2.2268, \t val-loss2.2329\n",
      "interation 100, train-loss 2.2217, \t val-loss2.2364\n",
      "interation 200, train-loss 2.2186, \t val-loss2.2368\n",
      "interation 300, train-loss 2.2161, \t val-loss2.2367\n",
      "interation 400, train-loss 2.2140, \t val-loss2.2365\n",
      "interation 500, train-loss 2.2122, \t val-loss2.2361\n",
      "interation 600, train-loss 2.2106, \t val-loss2.2358\n",
      "interation 700, train-loss 2.2091, \t val-loss2.2354\n",
      "interation 800, train-loss 2.2078, \t val-loss2.2351\n",
      "interation 900, train-loss 2.2067, \t val-loss2.2347\n",
      "interation 1000, train-loss 2.2056, \t val-loss2.2344\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "for i in range(1001):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train,0.0)\n",
    "    if i %100 == 0:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val,0.0)\n",
    "        print(f'interation {i}, train-loss {loss.item():.4f}, \\t val-loss{val_loss:.4f}')\n",
    "        \n",
    "    # backward_pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90109831-ec05-4604-b13f-cb022970d8e5",
   "metadata": {},
   "source": [
    "## E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "72783e67-1c7a-482a-b753-eeb46f334f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = torch.rand((703,27), requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b5204407-57d1-41d2-8371-3566749c1604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation 0, train-loss 3.3427, \t val-loss3.3423\n",
      "interation 100, train-loss 2.4252, \t val-loss2.4453\n",
      "interation 200, train-loss 2.3348, \t val-loss2.3575\n",
      "interation 300, train-loss 2.2960, \t val-loss2.3202\n",
      "interation 400, train-loss 2.2740, \t val-loss2.2994\n",
      "interation 500, train-loss 2.2596, \t val-loss2.2861\n",
      "interation 600, train-loss 2.2495, \t val-loss2.2769\n",
      "interation 700, train-loss 2.2419, \t val-loss2.2702\n",
      "interation 800, train-loss 2.2361, \t val-loss2.2650\n",
      "interation 900, train-loss 2.2314, \t val-loss2.2610\n",
      "interation 1000, train-loss 2.2275, \t val-loss2.2578\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    logits = W[xs_train]\n",
    "    loss = F.cross_entropy(logits, ys_train, label_smoothing = 0.001)\n",
    "    if i %100 == 0:\n",
    "        val_logits = W[xs_val]\n",
    "        val_loss = F.cross_entropy(val_logits, ys_val, label_smoothing = 0.001)\n",
    "        print(f'interation {i}, train-loss {loss.item():.4f}, \\t val-loss{val_loss:.4f}')\n",
    "        \n",
    "    # backward_pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d80e4e-abf2-4065-b7e8-8103d71cb980",
   "metadata": {},
   "source": [
    "## E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf1510-3f4c-40e3-9b99-47aaccd93f5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train the trigram model on half of the dataset using the loss cross_entropy and check the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "cc58a20b-8dfa-4eb2-bb4a-26bba45e99b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "half_idx , val_idx = torch.utils.data.random_split(range(xs.shape[0]),[0.5,0.5])\n",
    "\n",
    "xs_half = xs[half_idx]\n",
    "ys_half = ys[half_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c751e33d-6d49-4bfa-ae92-6ece09a3ea38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([114073]), torch.Size([114073]))"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_half.shape, ys_half.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f7969427-0507-458b-b836-e6997df4e599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = torch.rand((703,27), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "5365386f-27d2-44c5-b9fd-eb25fcec3700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation 0, loss 3.3330\n",
      "interation 100, loss 2.4295\n",
      "interation 200, loss 2.3371\n",
      "interation 300, loss 2.2975\n",
      "interation 400, loss 2.2750\n",
      "interation 500, loss 2.2604\n",
      "interation 600, loss 2.2501\n",
      "interation 700, loss 2.2424\n",
      "interation 800, loss 2.2365\n",
      "interation 900, loss 2.2317\n",
      "interation 1000, loss 2.2278\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    logits = W[xs_train]\n",
    "    loss = F.cross_entropy(logits, ys_train, label_smoothing = 0.001)\n",
    "    if i %100 == 0:\n",
    "        print(f'interation {i}, loss {loss.item():.4f}')\n",
    "        \n",
    "    # backward_pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5a7e987b-79f1-419e-af04-4d1693f3b303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasid\n",
      "prelay\n",
      "adin\n",
      "kairritonian\n",
      "juwa\n",
      "kalania\n",
      "zabileniassibduinrwin\n",
      "lessiyanayla\n",
      "te\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    char1 = '.'\n",
    "    char2 = '.'\n",
    "    \n",
    "    while True: \n",
    "        logits = W[ptoi[char1, char2]]\n",
    "        p = F.softmax(logits, dim = 0) # probabilities for next character\n",
    "        ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        char1 = char2\n",
    "        char2 = itos[ix]\n",
    "        out.append(itos[ix])\n",
    "        \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87b82eb3-250a-4303-bafd-9a6990a9b425",
   "metadata": {},
   "source": [
    "# prediction from full dataset using the nll loss\n",
    "junide\n",
    "janasid\n",
    "prelay\n",
    "adin\n",
    "kairritoper\n",
    "sathen\n",
    "sameia\n",
    "yanileniassibduinewin\n",
    "lessiyanayla\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913c632-c999-4918-814b-1c6398bffbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
